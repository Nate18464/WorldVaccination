{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3a3f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data = pd.read_csv(\"../../resource/ModelCreation/worldVaccinesCleanSimple.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7aad1d",
   "metadata": {},
   "source": [
    "Make date into integer of days since the first date in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb0b8b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.date = pd.to_datetime(data.date,format=\"%Y-%m-%d\") # Convert date strings to datetime objects\n",
    "mindate1 = data.date.min() # Save min for when we take in input\n",
    "data.date = data.date - data.date.min()\n",
    "data.date = pd.Series([x.days for x in data.date])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06674ffc",
   "metadata": {},
   "source": [
    "Make people_fully_vaccinated_per_hundred between the range of 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73bff2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"people_fully_vaccinated_per_hundred\"] = data[\"people_fully_vaccinated_per_hundred\"]/100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51794e13",
   "metadata": {},
   "source": [
    "Put data into a dictionary with keys as countries and a dictionary with the data as the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ba9ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadict = dict()\n",
    "for c in data.country.unique():\n",
    "    tmpdata = data.loc[data.country==c] # Get the data where the country column equals the certain country\n",
    "    tmpdata.drop(\"country\",axis=1,inplace=True) # Drop the country column because this value will be the same for this table\n",
    "    datadict[c] = {\"data\":tmpdata} # Set the value for the key at country c to be the data we just extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58341598",
   "metadata": {},
   "source": [
    "First, let's compare how my original Linear Regression model with transformations to turn it into logistic stacks up against a 1 neuron neural network with a sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5983e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel:\n",
    "    # Initialize data with trainX and triany, then find the correct equation to regress on\n",
    "    def __init__(self,trainX,trainy,testX,testy):\n",
    "        # By default, save all the train and test X and y as a fully random split of X and y\n",
    "        self.trainX = trainX\n",
    "        self.trainy = trainy\n",
    "        self.testX = testX\n",
    "        self.testy = testy\n",
    "        self.fit()\n",
    "        \n",
    "    def fit(self):\n",
    "        trainy = self.transformYFit(self.trainy.copy()) # Transform training x values\n",
    "        testy = self.transformYFit(self.testy.copy()) # Transform testing x values\n",
    "        self.model = LinearRegression().fit(self.trainX,trainy) # Make a model fit with the training x and y values\n",
    "        self.score = self.model.score(self.testX,testy)\n",
    "        \n",
    "    def transformYFit(self, Y):\n",
    "        # Create a lambda with formula to transform values.\n",
    "        transformation = lambda y: -1*math.log((1/(y+.01))-1)\n",
    "        return [transformation(y) for y in Y]\n",
    "    \n",
    "    def transformYPredict(self, Y):\n",
    "        transformation = lambda y: (1/(1+math.exp(-1*y)))-.01\n",
    "        return [transformation(y) for y in Y]\n",
    "    \n",
    "    def getScore(self):\n",
    "        return self.score\n",
    "        \n",
    "    def predict(self,X,y):\n",
    "        return (self.transformYPredict(self.model.predict(tmpX)), self.model.score(tmpX,y)) # Return the predicted y, but transformed to be betweeen 0 and 1, and the score\n",
    "    \n",
    "    def predictXOnly(self,X):\n",
    "        return self.transformYPredict(self.model.predict(X))\n",
    "    \n",
    "    def finalFit(self,X,y):\n",
    "        # Initialize training and test X and y to X and y.\n",
    "        self.trainX = X\n",
    "        self.testX = X\n",
    "        self.trainy = y\n",
    "        self.testy = y\n",
    "        self.fit(self.degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ab5ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolyLogisticRegressionModel:\n",
    "    # Initialize data with trainX and triany, then find the correct equation to regress on\n",
    "    def __init__(self,trainX,trainy,testX,testy):\n",
    "        # By default, save all the train and test X and y as a fully random split of X and y\n",
    "        self.trainX = trainX\n",
    "        self.trainy = trainy\n",
    "        self.testX = testX\n",
    "        self.testy = testy\n",
    "        self.findRegress()\n",
    "        \n",
    "    def findRegress(self):\n",
    "        # Set an initial model and score from fitting our model\n",
    "        self.model, self.score = self.fit(1)\n",
    "        self.degree = 1\n",
    "        # Loop through all possible boolean values\n",
    "        for d in range(1,10,2):\n",
    "            # Run the fit function\n",
    "            tmpmodel, tmpscore = self.fit(d)\n",
    "            # Check if this new model has a better score\n",
    "            if tmpscore > self.score:\n",
    "                # If the new model has a better score, update the model, score, and regress_bool\n",
    "                self.model = tmpmodel\n",
    "                self.score = tmpscore\n",
    "                self.degree = d\n",
    "        \n",
    "        \n",
    "    def fit(self, degree):\n",
    "        trainX = self.transformX(self.trainX.copy(), degree) # Transform training x values\n",
    "        testX = self.transformX(self.testX.copy(), degree) # Transform testing x values\n",
    "        trainy = self.transformYFit(self.trainy.copy()) # Transform training x values\n",
    "        testy = self.transformYFit(self.testy.copy()) # Transform testing x values\n",
    "        model = LinearRegression().fit(trainX,trainy) # Make a model fit with the training x and y values\n",
    "        score = model.score(testX,testy)\n",
    "        return model, score\n",
    "        \n",
    "    def transformYFit(self, Y):\n",
    "        # Create a lambda with formula to transform values.\n",
    "        transformation = lambda y: -1*math.log((1/(y+.01))-1)\n",
    "        return [transformation(y) for y in Y]\n",
    "    \n",
    "    def transformYPredict(self, Y):\n",
    "        transformation = lambda y: (1/(1+math.exp(-1*y)))-.01\n",
    "        for y in Y:\n",
    "            try:\n",
    "                if y > -700:\n",
    "                    transformation(y)\n",
    "            except:\n",
    "                print(y)\n",
    "        return [transformation(y) if y > -700 else 0 for y in Y]\n",
    "    \n",
    "    def transformX(self, X, degree):\n",
    "        # Create a list of lambdas with formulas to transform values.\n",
    "        newX = []\n",
    "        # Loop through each row in the list\n",
    "        for row in X:\n",
    "            new_values = []\n",
    "            # Loop through each possible transformation\n",
    "            if degree == 9:\n",
    "                for val in row:\n",
    "                    new_values.append(math.log(val+2))\n",
    "                newX.append(np.append(row, new_values))\n",
    "            else: \n",
    "                for p in range(2,degree+1):\n",
    "                    for val in row:\n",
    "                        new_values.append(val**p)\n",
    "                newX.append(np.append(row, new_values))\n",
    "        return newX\n",
    "    \n",
    "    def getScore(self):\n",
    "        return self.score\n",
    "    \n",
    "    def getDegree(self):\n",
    "        return self.degree\n",
    "        \n",
    "    def predict(self,X,y):\n",
    "        tmpX = self.transformX(X,self.degree)\n",
    "        return (self.transformYPredict(self.model.predict(tmpX)), self.model.score(tmpX,y)) # Return the predicted y, but transformed to be betweeen 0 and 1, and the score\n",
    "    \n",
    "    def predictXOnly(self,X):\n",
    "        return self.transformYPredict(self.model.predict(self.transformX(X,self.degree)))\n",
    "    \n",
    "    def finalFit(self,X,y):\n",
    "        # Initialize training and test X and y to X and y.\n",
    "        self.trainX = X\n",
    "        self.testX = X\n",
    "        self.trainy = y\n",
    "        self.testy = y\n",
    "        self.fit(self.degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cba281e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States NN: 0.18020727 Reg: 0.038589094\n"
     ]
    }
   ],
   "source": [
    "c = \"United States\"\n",
    "X = datadict[c][\"data\"][\"date\"].to_numpy().reshape(-1,1)\n",
    "y = datadict[c][\"data\"][\"people_fully_vaccinated_per_hundred\"] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2)\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "modelReg = RegressionModel(trainX=X_train,testX=X_test,trainy=y_train,testy=y_test)\n",
    "y_predReg = modelReg.predictXOnly(X_test)\n",
    "modelNN = keras.Sequential() # Create Model\n",
    "modelNN.add(layers.Dense(1, input_dim = 1,activation=\"sigmoid\")) # Add Input Layer\n",
    "modelNN.compile(loss = 'mae', optimizer='sgd')\n",
    "result = modelNN.fit(X_train,y_train,epochs=10000,batch_size=5,verbose=0,callbacks=[callback])\n",
    "y_predNN = modelNN.predict(X_test)\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "datadict[c][\"lossNN\"] = mae(y_test, y_predNN).numpy()\n",
    "datadict[c][\"lossReg\"] = mae(y_test,y_predReg).numpy()\n",
    "print(c, \"NN:\",datadict[c][\"lossNN\"], \"Reg:\",datadict[c][\"lossReg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0a9fb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States Batch: 5\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
    "X = datadict[c][\"data\"][\"date\"].to_numpy().reshape(-1,1)\n",
    "datadict[c][\"X\"] = MinMaxScaler(feature_range=(-1,1)).fit_transform(X)\n",
    "datadict[c][\"y\"] = datadict[c][\"data\"][\"people_fully_vaccinated_per_hundred\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(datadict[c][\"X\"],datadict[c][\"y\"],test_size=.2)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train,y_train,test_size=.2*10/8)\n",
    "datadict[c][\"batch\"] = 5\n",
    "datadict[c][\"model\"] = keras.Sequential() # Create Model\n",
    "datadict[c][\"model\"].add(layers.Dense(1, input_dim = 1,activation=\"sigmoid\")) # Add Input Layer\n",
    "datadict[c][\"model\"].compile(loss = 'mae', optimizer='sgd')\n",
    "datadict[c][\"result\"] = datadict[c][\"model\"].fit(X_train,y_train,epochs=10000,batch_size=5,verbose=0,callbacks=[callback])\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "y_pred = datadict[c][\"model\"].predict(X_valid)\n",
    "datadict[c][\"loss\"] = mae(y_valid, y_pred).numpy()\n",
    "for b in range(10,55,5):\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
    "    model = keras.Sequential() # Create Model\n",
    "    model.add(layers.Dense(1, input_dim = 1,activation=\"sigmoid\")) # Add Input Layer\n",
    "    model.compile(loss = 'mae', optimizer='sgd')\n",
    "    result = model.fit(X_train,y_train,epochs=10000,batch_size=b,verbose=0,callbacks=[callback])\n",
    "    y_pred = datadict[c][\"model\"].predict(X_valid)\n",
    "    loss = mae(y_valid, y_pred).numpy()\n",
    "    if loss < datadict[c][\"loss\"]:\n",
    "        datadict[c][\"model\"] = model\n",
    "        datadict[c][\"result\"] = result\n",
    "        datadict[c][\"loss\"] = loss\n",
    "        datadict[c][\"batch\"] = b\n",
    "print(c, \"Batch:\",datadict[c][\"batch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0103cdeb",
   "metadata": {},
   "source": [
    "From this testing, we see that batch_size of 5 is best, so we will just hard code this in from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc17adf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'hidden'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-caf374181d99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mdatadict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hidden\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mdatadict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"size\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatadict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hidden\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatadict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'hidden'"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
    "X = datadict[c][\"data\"][\"date\"].to_numpy().reshape(-1,1)\n",
    "datadict[c][\"X\"] = MinMaxScaler(feature_range=(-1,1)).fit_transform(X)\n",
    "datadict[c][\"y\"] = datadict[c][\"data\"][\"people_fully_vaccinated_per_hundred\"]\n",
    "X_train, datadict[c][\"X_test\"], y_train, datadict[c][\"y_test\"] = train_test_split(datadict[c][\"X\"],datadict[c][\"y\"],test_size=.2)\n",
    "datadict[c][\"X_train\"], datadict[c][\"X_valid\"], datadict[c][\"y_train\"], datadict[c][\"y_valid\"] = train_test_split(X_train,y_train,test_size=.2*10/8)\n",
    "datadict[c][\"model\"] = keras.Sequential() # Create Model\n",
    "datadict[c][\"model\"].add(layers.Dense(16, input_dim = 1,activation=\"sigmoid\")) # Add Input Layer\n",
    "datadict[c][\"model\"].add(layers.Dense(1,activation=\"sigmoid\"))\n",
    "datadict[c][\"model\"].compile(loss = 'mae', optimizer='sgd')\n",
    "datadict[c][\"result\"] = datadict[c][\"model\"].fit(datadict[c][\"X_train\"],datadict[c][\"y_train\"],epochs=10000,batch_size=5,verbose=0,callbacks=[callback])\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "y_pred = datadict[c][\"model\"].predict(datadict[c][\"X_valid\"])\n",
    "datadict[c][\"loss\"] = mae(datadict[c][\"y_valid\"], y_pred).numpy()\n",
    "datadict[c][\"hidden\"] = 0\n",
    "datadict[c][\"size\"] = 1\n",
    "for h in range(4):\n",
    "    for s in range(1,17):\n",
    "        model = keras.Sequential() # Create Model\n",
    "        model.add(layers.Dense(s, input_dim = 1,activation=\"sigmoid\")) # Add Input Layer\n",
    "        for _ in range(h):\n",
    "            model.add(layers.Dense(s,activation=\"sigmoid\"))\n",
    "        model.add(layers.Dense(1,activation=\"sigmoid\"))\n",
    "        model.compile(loss = 'mae', optimizer='sgd')\n",
    "        result = model.fit(datadict[c][\"X_train\"],datadict[c][\"y_train\"],epochs=10000,batch_size=5,verbose=0,callbacks=[callback])\n",
    "        y_pred = datadict[c][\"model\"].predict(X_valid)\n",
    "        loss = mae(datadict[c][\"y_valid\"], y_pred).numpy()\n",
    "        if loss < datadict[c][\"loss\"]:\n",
    "            datadict[c][\"model\"] = model\n",
    "            datadict[c][\"result\"] = result\n",
    "            datadict[c][\"loss\"] = loss\n",
    "            datadict[c][\"hidden\"] = h\n",
    "            datadict[c][\"size\"] = s\n",
    "print(datadict[c][\"hidden\"], datadict[c][\"size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d093107",
   "metadata": {},
   "source": [
    "From this, we see that it is better to just have the one input layer and output, and just 16 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "027ca3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb5041969d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "United States NN: 0.17358632 Reg: 0.044319764\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
    "X = datadict[c][\"data\"][\"date\"].to_numpy().reshape(-1,1)\n",
    "y = datadict[c][\"data\"][\"people_fully_vaccinated_per_hundred\"] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2)\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "modelReg = RegressionModel(trainX=X_train,testX=X_test,trainy=y_train,testy=y_test)\n",
    "y_predReg = modelReg.predictXOnly(X_test)\n",
    "modelNN = keras.Sequential() # Create Model\n",
    "modelNN.add(layers.Dense(16, input_dim = 1,activation=\"sigmoid\")) # Add Input Layer\n",
    "modelNN.add(layers.Dense(1,activation=\"sigmoid\"))\n",
    "modelNN.compile(loss = 'mae', optimizer='sgd')\n",
    "result = modelNN.fit(X_train,y_train,epochs=10000,batch_size=5,verbose=0,callbacks=[callback])\n",
    "y_predNN = modelNN.predict(X_test)\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "datadict[c][\"lossNN\"] = mae(y_test, y_predNN).numpy()\n",
    "datadict[c][\"lossReg\"] = mae(y_test,y_predReg).numpy()\n",
    "    \n",
    "print(c, \"NN:\",datadict[c][\"lossNN\"], \"Reg:\",datadict[c][\"lossReg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a56a305",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0c72645fbc5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodelNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Add Input Layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodelNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mae'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sgd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0my_predNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeanAbsoluteError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_build_call_outputs\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m   2169\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2170\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2171\u001b[0;31m         \u001b[0mcustom_gradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2172\u001b[0m         \u001b[0moutputs_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m         \u001b[0mj\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
    "for c in datadict.keys():\n",
    "    X = datadict[c][\"data\"][\"date\"].to_numpy().reshape(-1,1)\n",
    "    y = datadict[c][\"data\"][\"people_fully_vaccinated_per_hundred\"] \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2)\n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    modelReg = RegressionModel(trainX=X_train,testX=X_test,trainy=y_train,testy=y_test)\n",
    "    y_predReg = modelReg.predictXOnly(X_test)\n",
    "    modelNN = keras.Sequential() # Create Model\n",
    "    modelNN.add(layers.Dense(1, input_dim = 1,activation=\"sigmoid\")) # Add Input Layer\n",
    "    modelNN.compile(loss = 'mae', optimizer='sgd')\n",
    "    result = modelNN.fit(X_train,y_train,epochs=10000,batch_size=5,verbose=0,callbacks=[callback])\n",
    "    y_predNN = modelNN.predict(X_test)\n",
    "    mae = tf.keras.losses.MeanAbsoluteError()\n",
    "    datadict[c][\"lossNN\"] = mae(y_test, y_predNN).numpy()\n",
    "    datadict[c][\"lossReg\"] = mae(y_test,y_predReg).numpy()\n",
    "\n",
    "for c in datadict.keys():\n",
    "    print(c, \"NN:\",datadict[c][\"lossNN\"], \"Reg:\",datadict[c][\"lossReg\"])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a455c3",
   "metadata": {},
   "source": [
    "From this we can see that just using a NN to try to create a logistic curve results in far worse mae. However, now we will test on different sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b9a5ef",
   "metadata": {},
   "source": [
    "Create ANN test what a good batch_size is, using a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e5b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
    "for c in datadict.keys():\n",
    "    X = datadict[c][\"data\"][\"date\"].to_numpy().reshape(-1,1)\n",
    "    datadict[c][\"X\"] = MinMaxScaler(feature_range=(-1,1)).fit_transform(X)\n",
    "    datadict[c][\"y\"] = datadict[c][\"data\"][\"people_fully_vaccinated_per_hundred\"]\n",
    "    X_train, datadict[c][\"X_test\"], y_train, datadict[c][\"y_test\"] = train_test_split(datadict[c][\"X\"],datadict[c][\"y\"],test_size=.2)\n",
    "    datadict[c][\"X_train\"], datadict[c][\"X_valid\"], datadict[c][\"y_train\"], datadict[c][\"y_valid\"] = train_test_split(X_train,y_train,test_size=.2*10/8)\n",
    "    datadict[c][\"batch\"] = 5\n",
    "    datadict[c][\"model\"] = keras.Sequential() # Create Model\n",
    "    datadict[c][\"model\"].add(layers.Dense(1, input_dim = 1,activation=\"sigmoid\")) # Add Input Layer\n",
    "    datadict[c][\"model\"].compile(loss = 'mae', optimizer='sgd')\n",
    "    datadict[c][\"result\"] = datadict[c][\"model\"].fit(datadict[c][\"X_train\"],datadict[c][\"y_train\"],epochs=10000,batch_size=5,verbose=0,callbacks=[callback])\n",
    "    mae = tf.keras.losses.MeanAbsoluteError()\n",
    "    y_pred = datadict[c][\"model\"].predict(X_valid)\n",
    "    datadict[c][\"loss\"] = mae(y_valid, y_pred).numpy()\n",
    "    for b in range(10,55,5):\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
    "        model = keras.Sequential() # Create Model\n",
    "        model.add(layers.Dense(1, input_dim = 1,activation=\"sigmoid\")) # Add Input Layer\n",
    "        model.compile(loss = 'mae', optimizer='sgd')\n",
    "        result = model.fit(datadict[c][\"X_train\"],datadict[c][\"y_train\"],epochs=10000,batch_size=b,verbose=0,callbacks=[callback])\n",
    "        y_pred = datadict[c][\"model\"].predict(X_valid)\n",
    "        loss = mae(datadict[c][\"y_valid\"], datadict[c][\"y_pred\"]).numpy()\n",
    "        if loss < datadict[c][\"loss\"]:\n",
    "            datadict[c][\"model\"] = model\n",
    "            datadict[c][\"result\"] = result\n",
    "            datadict[c][\"loss\"] = loss\n",
    "            datadict[c][\"batch\"] = b\n",
    "    print(c, \"NN:\",datadict[c][\"lossNN\"], \"Reg:\",datadict[c][\"lossReg\"])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07418ef",
   "metadata": {},
   "source": [
    "From this testing, we see that batch_size of 5 is best, so we will just hard code this in from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd88038",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
    "for c in datadict.keys():\n",
    "    X = datadict[c][\"data\"][\"date\"].to_numpy().reshape(-1,1)\n",
    "    datadict[c][\"X\"] = MinMaxScaler(feature_range=(-1,1)).fit_transform(X)\n",
    "    datadict[c][\"y\"] = datadict[c][\"data\"][\"people_fully_vaccinated_per_hundred\"]\n",
    "    X_train, datadict[c][\"X_test\"], y_train, datadict[c][\"y_test\"] = train_test_split(datadict[c][\"X\"],datadict[c][\"y\"],test_size=.2)\n",
    "    datadict[c][\"X_train\"], datadict[c][\"X_valid\"], datadict[c][\"y_train\"], datadict[c][\"y_valid\"] = train_test_split(X_train,y_train,test_size=.2*10/8)\n",
    "    datadict[c][\"model\"] = keras.Sequential() # Create Model\n",
    "    datadict[c][\"model\"].add(layers.Dense(16, input_dim = 1,activation=\"sigmoid\")) # Add Input Layer\n",
    "    datadict[c][\"model\"].add(layers.Dense(1,activation=\"sigmoid\"))\n",
    "    datadict[c][\"model\"].compile(loss = 'mae', optimizer='sgd')\n",
    "    datadict[c][\"result\"] = datadict[c][\"model\"].fit(datadict[c][\"X_train\"],datadict[c][\"y_train\"],epochs=10000,batch_size=5,verbose=0,callbacks=[callback])\n",
    "    mae = tf.keras.losses.MeanAbsoluteError()\n",
    "    y_pred = datadict[c][\"model\"].predict(datadict[c][\"X_valid\"])\n",
    "    datadict[c][\"loss\"] = mae(datadict[c][\"y_valid\"], y_pred).numpy()\n",
    "    for h in range(4):\n",
    "        for s in range(1,17):\n",
    "            model = keras.Sequential() # Create Model\n",
    "            model.add(layers.Dense(s, input_dim = 1,activation=\"sigmoid\")) # Add Input Layer\n",
    "            for _ in range(h):\n",
    "                model.add(layers.Dense(s,activation=\"sigmoid\"))\n",
    "            model.add(layers.Dense(1,activation=\"sigmoid\"))\n",
    "            model.compile(loss = 'mae', optimizer='sgd')\n",
    "            result = model.fit(datadict[c][\"X_train\"],datadict[c][\"y_train\"],epochs=10000,batch_size=5,verbose=0,callbacks=[callback])\n",
    "            y_pred = datadict[c][\"model\"].predict(X_valid)\n",
    "            loss = mae(datadict[c][\"y_valid\"], y_pred).numpy()\n",
    "            if loss < datadict[c][\"loss\"]:\n",
    "                datadict[c][\"model\"] = model\n",
    "                datadict[c][\"result\"] = result\n",
    "                datadict[c][\"loss\"] = loss\n",
    "                datadict[c][\"hidden\"] = h\n",
    "                datadict[c][\"size\"] = s\n",
    "    print(datadict[c][\"hidden\"], datadict[c][\"size\"])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07694af9",
   "metadata": {},
   "source": [
    "From this, we see that it is better to just have the one input layer and output, and just 16 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08a7836",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
    "for c in datadict.keys():\n",
    "    X = datadict[c][\"data\"][\"date\"].to_numpy().reshape(-1,1)\n",
    "    y = datadict[c][\"data\"][\"people_fully_vaccinated_per_hundred\"] \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2)\n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    modelReg = RegressionModel(trainX=X_train,testX=X_test,trainy=y_train,testy=y_test)\n",
    "    y_predReg = modelReg.predictXOnly(X_test)\n",
    "    modelNN = keras.Sequential() # Create Model\n",
    "    modelNN.add(layers.Dense(16, input_dim = 1,activation=\"sigmoid\")) # Add Input Layer\n",
    "    modelNN.add(layers.Dense(1,activation=\"sigmoid\"))\n",
    "    modelNN.compile(loss = 'mae', optimizer='sgd')\n",
    "    result = modelNN.fit(X_train,y_train,epochs=10000,batch_size=5,verbose=0,callbacks=[callback])\n",
    "    y_predNN = modelNN.predict(X_test)\n",
    "    mae = tf.keras.losses.MeanAbsoluteError()\n",
    "    datadict[c][\"lossNN\"] = mae(y_test, y_predNN).numpy()\n",
    "    datadict[c][\"lossReg\"] = mae(y_test,y_predReg).numpy()\n",
    "    \n",
    "for c in datadict.keys():\n",
    "    print(c, \"NN:\",datadict[c][\"lossNN\"], \"Reg:\",datadict[c][\"lossReg\"])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77660a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
